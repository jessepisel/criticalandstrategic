{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset imputations\n",
    "## First we will impute the rock chip samples and assign the values to the hydraulic unit basins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this you need the following data: rock chip samples in `.csv` format and a HUC basin shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import statsmodels as sm\n",
    "from statsmodels.imputation import mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12 = gpd.read_file(\n",
    "    r\"huc_12.shp\"\n",
    ")  # reads in the hydraulic basin units and adds an area column\n",
    "huc12 = huc12.to_crs({\"init\": \"epsg:3732\"})\n",
    "huc12.plot(column=\"HU_12_NAME\", figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# munge the rock chip data\n",
    "\n",
    "ref = pd.read_csv(r\"ngdbrock-fUS56\\tblRockGeoData.csv\", index_col=[0])\n",
    "path = r\"\\\\ngdbrock-fUS56\\\\ngdbrock-fUS56\\\\\"\n",
    "allFiles = glob.glob(path + \"*.csv\")\n",
    "analysis = pd.DataFrame\n",
    "moving = []\n",
    "for file in allFiles:\n",
    "    types = pd.read_csv(file)\n",
    "    moving.append(types)\n",
    "analysis = pd.concat(moving, sort=False)\n",
    "\n",
    "\n",
    "merged = ref.merge(analysis, on=\"lab_id\")\n",
    "merged.drop(\n",
    "    columns=[\"br_ppm\", \"ir_ppm\", \"rn_ppm\", \"i_ppm\", \"ir_ppb\", \"au_ppb\"],\n",
    "    inplace=True,\n",
    ")\n",
    "merged[\"tracking\"] = np.arange(0, len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where we do the imputation\n",
    "imp = sm.imputation.mice.MICEData(\n",
    "    merged.iloc[:, 32:-3], perturbation_method=\"gaussian\", k_pmm=2\n",
    ")\n",
    "for j in tqdm(range(200)):\n",
    "    imp.update_all()\n",
    "    if j % 10 == 0:\n",
    "        print(j)\n",
    "        imp.data.to_csv(\"iteration\" + str(j) + \".csv\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "imp.to_csv(r\"\\imputed_rock_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_merged = merged[\n",
    "    [\n",
    "        \"lab_id\",\n",
    "        \"job_id\",\n",
    "        \"submitter\",\n",
    "        \"date_sub\",\n",
    "        \"date_sub2\",\n",
    "        \"field_id\",\n",
    "        \"state\",\n",
    "        \"country\",\n",
    "        \"datum\",\n",
    "        \"spheroid\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"depth\",\n",
    "        \"locat_desc\",\n",
    "        \"datecollct\",\n",
    "        \"sample_src\",\n",
    "        \"methcollct\",\n",
    "        \"primeclass\",\n",
    "        \"xndryclass\",\n",
    "        \"spec_name\",\n",
    "        \"addl_attr\",\n",
    "        \"geol_age\",\n",
    "        \"stratgrphy\",\n",
    "        \"mineralztn\",\n",
    "        \"alteration\",\n",
    "        \"struct_src\",\n",
    "        \"dep_envirn\",\n",
    "        \"source_rk\",\n",
    "        \"metamrphsm\",\n",
    "        \"facies_grd\",\n",
    "        \"prep\",\n",
    "        \"mesh_size\",\n",
    "        \"tracking\",\n",
    "    ]\n",
    "].merge(imp.data, on=\"tracking\")\n",
    "# building the dataset out with the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "\n",
    "imputed_merged = pd.read_csv(\n",
    "    r\"\\imputed_rock_values.csv\"\n",
    ")  # reads in the point data\n",
    "\n",
    "# writes the point data to a shapefile in the dir called data.shp\n",
    "geometry = [\n",
    "    Point(xy) for xy in zip(imputed_merged.longitude, imputed_merged.latitude)\n",
    "]\n",
    "crs = {\"init\": \"epsg:3732\"}  # http://www.spatialreference.org/ref/epsg/2263/\n",
    "geo_df = GeoDataFrame(\n",
    "    imputed_merged, crs={\"init\": \"epsg:4326\"}, geometry=geometry\n",
    ")\n",
    "\n",
    "geo_df.to_file(driver=\"ESRI Shapefile\", filename=\"rock_data.shp\")\n",
    "projGeoDF = geo_df.to_crs(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the point data and decluster with max values to the huc basin\n",
    "\n",
    "points_with_basin = gpd.sjoin(huc12, projGeoDF, how=\"inner\", op=\"intersects\")\n",
    "maximal = points_with_basin.dissolve(by=\"HUC_12\", aggfunc=\"max\")\n",
    "maximal = maximal.drop(\n",
    "    columns=[\n",
    "        \"AREA\",\n",
    "        \"PERIMETER\",\n",
    "        \"WY_HU12_\",\n",
    "        \"WY_HU12_ID\",\n",
    "        \"HUC_8\",\n",
    "        \"HUC_10\",\n",
    "        \"ACRES\",\n",
    "        \"STATES\",\n",
    "        \"NCONTRB_A\",\n",
    "        \"HU_10_DS\",\n",
    "        \"HU_10_NAME\",\n",
    "        \"HU_10_MOD\",\n",
    "        \"HU_10_TYPE\",\n",
    "        \"HU_12_DS\",\n",
    "        \"HU_12_NAME\",\n",
    "        \"HU_12_MOD\",\n",
    "        \"HU_12_TYPE\",\n",
    "        \"HU_2_NAME\",\n",
    "        \"HU_4_NAME\",\n",
    "        \"HU_6_NAME\",\n",
    "        \"HU_8_NAME\",\n",
    "        \"Shape_Leng\",\n",
    "        \"Shape_Area\",\n",
    "        \"index_right\",\n",
    "    ]\n",
    ")\n",
    "maximal.to_file(driver=\"ESRI Shapefile\", filename=\"max_rock_values.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This next section deals with imputation of the sediment sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nure = pd.read_csv(r\"nure.csv\", encoding=\"iso-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataColumns = [\n",
    "    \"u_dn_ppm\",\n",
    "    \"u_fl_ppm\",\n",
    "    \"ag_ppm\",\n",
    "    \"al_pct\",\n",
    "    \"as_ppm\",\n",
    "    \"au_ppm\",\n",
    "    \"b_ppm\",\n",
    "    \"ba_ppm\",\n",
    "    \"be_ppm\",\n",
    "    \"bi_ppm\",\n",
    "    \"ca_pct\",\n",
    "    \"cd_ppm\",\n",
    "    \"ce_ppm\",\n",
    "    \"cl_ppm\",\n",
    "    \"co_ppm\",\n",
    "    \"cr_ppm\",\n",
    "    \"cs_ppm\",\n",
    "    \"cu_ppm\",\n",
    "    \"dy_ppm\",\n",
    "    \"eu_ppm\",\n",
    "    \"fe_pct\",\n",
    "    \"hf_ppm\",\n",
    "    \"k_pct\",\n",
    "    \"la_ppm\",\n",
    "    \"li_ppm\",\n",
    "    \"lu_ppm\",\n",
    "    \"mg_pct\",\n",
    "    \"mn_ppm\",\n",
    "    \"mo_ppm\",\n",
    "    \"na_pct\",\n",
    "    \"nb_ppm\",\n",
    "    \"ni_ppm\",\n",
    "    \"p_ppm\",\n",
    "    \"pb_ppm\",\n",
    "    \"rb_ppm\",\n",
    "    \"sb_ppm\",\n",
    "    \"sc_ppm\",\n",
    "    \"se_ppm\",\n",
    "    \"sm_ppm\",\n",
    "    \"sn_ppm\",\n",
    "    \"sr_ppm\",\n",
    "    \"ta_ppm\",\n",
    "    \"tb_ppm\",\n",
    "    \"th_ppm\",\n",
    "    \"ti_ppm\",\n",
    "    \"v_ppm\",\n",
    "    \"w_ppm\",\n",
    "    \"y_ppm\",\n",
    "    \"yb_ppm\",\n",
    "    \"zn_ppm\",\n",
    "    \"zr_ppm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = sm.imputation.mice.MICEData(nure[dataColumns])\n",
    "for j in tqdm(range(200)):\n",
    "    imp.update_all()\n",
    "    if j % 10 == 0:\n",
    "        print(j)\n",
    "        imp.data.to_csv(\"nure_iteration\" + str(j) + \".csv\")\n",
    "    else:\n",
    "        pass\n",
    "imp.to_csv(r\"\\imputed_nure_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nure = pd.read_csv(r\"\\imputed.csv\")  # reads in the point data\n",
    "\n",
    "# writes the point data to a shapefile in the dir called data.shp\n",
    "geometry = [Point(xy) for xy in zip(nure.longitude, nure.latitude)]\n",
    "crs = {\"init\": \"epsg:4326\"}\n",
    "geo_df = GeoDataFrame(nure, crs={\"init\": \"epsg:4326\"}, geometry=geometry)\n",
    "\n",
    "geo_df.to_file(driver=\"ESRI Shapefile\", filename=\"nure_data.shp\")\n",
    "projGeoDF = geo_df.to_crs(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_with_basin = gpd.sjoin(huc12, projGeoDF, how=\"inner\", op=\"intersects\")\n",
    "maximal = points_with_basin.dissolve(by=\"HUC_12\", aggfunc=\"max\")\n",
    "maximal = maximal.drop(\n",
    "    columns=[\n",
    "        \"AREA\",\n",
    "        \"PERIMETER\",\n",
    "        \"WY_HU12_\",\n",
    "        \"WY_HU12_ID\",\n",
    "        \"HUC_8\",\n",
    "        \"HUC_10\",\n",
    "        \"ACRES\",\n",
    "        \"STATES\",\n",
    "        \"NCONTRB_A\",\n",
    "        \"HU_10_DS\",\n",
    "        \"HU_10_NAME\",\n",
    "        \"HU_10_MOD\",\n",
    "        \"HU_10_TYPE\",\n",
    "        \"HU_12_DS\",\n",
    "        \"HU_12_NAME\",\n",
    "        \"HU_12_MOD\",\n",
    "        \"HU_12_TYPE\",\n",
    "        \"HU_2_NAME\",\n",
    "        \"HU_4_NAME\",\n",
    "        \"HU_6_NAME\",\n",
    "        \"HU_8_NAME\",\n",
    "        \"Shape_Leng\",\n",
    "        \"Shape_Area\",\n",
    "        \"index_right\",\n",
    "        \"Unnamed: 0\",\n",
    "    ]\n",
    ")\n",
    "maximal.to_file(driver=\"ESRI Shapefile\", filename=\"max_nure_values.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
