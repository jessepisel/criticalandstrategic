{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook calculates the *Gi** values for the sediment samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note rock sample *Gi** values were calculated in `ArcMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import geopandas as gpd\n",
    "import itertools, glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = gpd.read_file(r\"max_nure_values.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hucs = imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hucs[\"FID\"] = hucs.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = hucs.loc[0:].FID  # huc basin number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []  # downstream basin list\n",
    "for i in range(len(hucs)):\n",
    "    downstream = hucs[hucs[\"HUC_12\"] == hucs.loc[i].HU_12_DS].FID.values\n",
    "    ds.append(downstream)\n",
    "# huc basin downstream of huc basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = []  # upstream basin list\n",
    "for i in range(len(hucs)):\n",
    "    upstream = hucs.loc[hucs[\"HU_12_DS\"] == hucs.loc[i].HUC_12].FID.values\n",
    "    us.append(upstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lso = []  # combined upstream and downstream basins\n",
    "for i in range(len(ds)):\n",
    "    ls = []\n",
    "    if ds[i].size > 0:\n",
    "        ls.append(ds[i][0])\n",
    "    else:\n",
    "        ls.append(-1)\n",
    "    for j in range(len(us[i])):\n",
    "        if us[i].size > 0:\n",
    "            ls.append(us[i][j])\n",
    "        else:\n",
    "            ls.append(-1)\n",
    "    lso.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = dict(zip(keys, lso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calculates the Gi* values for the dendritic spatial network\n",
    "def GIstar(element):\n",
    "    Xbar = hucs[\n",
    "        element\n",
    "    ].mean()  # calculates the mean value of the element across all HUCS\n",
    "    S = np.sqrt(\n",
    "        (((hucs[element]) ** 2).sum() / len(hucs)) - (Xbar ** 2)\n",
    "    )  # calculates the S value in the denominator\n",
    "    import scipy.sparse as sp  # import bruh\n",
    "\n",
    "    mat = sp.dok_matrix(\n",
    "        (len(hucs), len(hucs)), dtype=np.int8\n",
    "    )  # creates a sparse matrix based on a dictionary of keys from above\n",
    "    for (\n",
    "        huc_a,\n",
    "        huc_b,\n",
    "    ) in (\n",
    "        neighbors.items()\n",
    "    ):  # this creates a sparse matrix of spatial contiguity weights\n",
    "        mat[huc_a, huc_b] = 1\n",
    "        mat = mat.transpose().tocsr()\n",
    "    z_score = []\n",
    "    for i in tqdm(\n",
    "        range(len(hucs))\n",
    "    ):  # here is the G* function that calculates the z-score\n",
    "        try:\n",
    "            row_standardized = mat.toarray()[i] / mat.toarray()[i].sum()\n",
    "            numerator = ((hucs[element].values * row_standardized).sum()) - (\n",
    "                row_standardized.sum() * Xbar\n",
    "            )  # numerator\n",
    "            denominator = (\n",
    "                np.sqrt(\n",
    "                    abs(\n",
    "                        ((row_standardized ** 2).sum())\n",
    "                        - (row_standardized.sum()) ** 2\n",
    "                    )\n",
    "                    / (len(row_standardized) - 1)\n",
    "                )\n",
    "                * S\n",
    "            )  # denominator\n",
    "            z_score.append(numerator / denominator)\n",
    "        except:\n",
    "            z_score.append(np.nan)\n",
    "    hucs[\"z_score\"] = z_score\n",
    "    hucs[\"z_score\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    hucs[\"z_score\"].fillna(0, inplace=True)\n",
    "    G = hucs[[element, \"HUC_12\", \"z_score\", \"geometry\"]]\n",
    "    G.to_file(\n",
    "        driver=\"ESRI Shapefile\",\n",
    "        filename=\"./drainage hotspots/\" + str(element) + \"_hotspots.shp\",\n",
    "    )\n",
    "    H = hucs[[element, \"HUC_12\", \"z_score\"]]\n",
    "    H.to_csv(r\"\\\\sed hotspot joined with hucs\\\\\" + str(element) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Gi* for all elements in the sediment dataset\n",
    "element_list = hucs.columns[26:-6].values\n",
    "for thing in element_list:\n",
    "    GIstar(thing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
