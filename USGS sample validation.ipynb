{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse.pisel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (12,14,26,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r'J:\\Geology\\WSGS\\Projects\\Critical Minerals\\ngdbrock-fUS56\\ngdbrock-fUS56\\xtbIcpaesChem.csv').fillna(0)\n",
    "ref = pd.read_csv(r'J:\\Geology\\WSGS\\Projects\\Critical Minerals\\ngdbrock-fUS56\\tblRockGeoData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12=gpd.read_file(r'J:\\Geology\\WSGS\\GIS\\huc_12.shp') #reads in the hydraulic basin units and adds an area column\n",
    "huc12= huc12.to_crs({'init': 'epsg:3732'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "#writes the point data to a shapefile in the dir called data.shp\n",
    "geometry = [Point(xy) for xy in zip(ref.longitude, ref.latitude)]\n",
    "crs = {'init': 'epsg:3732'} \n",
    "geo_df = GeoDataFrame(ref, crs={'init': 'epsg:4326'}, geometry=geometry)\n",
    "#geo_df.to_file(driver='ESRI Shapefile', filename='data.shp')\n",
    "projGeoDF=geo_df.to_crs(crs)\n",
    "val_huc = gpd.sjoin(huc12, projGeoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_huc = gpd.sjoin(huc12, projGeoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_huc = val_huc.merge(data, on='lab_id') #this is the chemical data joined with the HUC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'J:\\\\Geology\\\\WSGS\\\\Projects\\\\Critical Minerals\\\\probability maps\\\\'\n",
    "allFiles=glob.glob(path+\"*.csv\")\n",
    "frame=pd.DataFrame\n",
    "listed=[]\n",
    "for file in allFiles:\n",
    "    df = pd.read_csv(file, index_col=[0])\n",
    "    listed.append(df)\n",
    "frame = pd.concat(listed)\n",
    "frame['HUC_12']=frame.HUC_12.astype(str)\n",
    "CaptainHucstable = huc12.merge(frame, on='HUC_12' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1042/1042 [00:00<00:00, 4302.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [06:25<00:00, 96.37s/it]\n"
     ]
    }
   ],
   "source": [
    "true_vals = []\n",
    "for samples in tqdm(range(len(val_huc))):\n",
    "    true_vals.append(val_huc.loc[samples].HUC_12)\n",
    "\n",
    "bflat = []  \n",
    "for samples in tqdm(range(4)):#len(val_huc))):\n",
    "    formation = val_huc.loc[samples].stratgrphy[0:10]\n",
    "    b = []\n",
    "    for element in range(58,113):\n",
    "        ppm = val_huc.columns[element] #select each element in the list\n",
    "        valued = val_huc.loc[samples][element] #select each elements values \n",
    "        #from the big list, select element, formation, and concentration greater than the sample has, and create a list of the\n",
    "        #basins that we should explore\n",
    "        b0 = CaptainHucstable[(CaptainHucstable['element']== ppm) & (CaptainHucstable['formation'].str.contains(formation)) &(\n",
    "            CaptainHucstable['bins_ppm']>= valued) & (CaptainHucstable['prob']>0.1)].HUC_12.values\n",
    "        b.append(b0)\n",
    "    bflat.append(np.unique(np.asarray([item for sublist in b for item in sublist])))\n",
    "preds = np.unique(np.asarray([item for sublist in bflat for item in sublist]))\n",
    "TP = len(set(true_vals).intersection(preds))\n",
    "FP = len(preds)-len(set(true_vals).intersection(preds))\n",
    "TN = len(huc12.HUC_12.unique())-len(preds)\n",
    "FN = len(set(true_vals).intersection(set(huc12.HUC_12.unique()) ^ set(preds)))\n",
    "\n",
    "#precision = TP/(TP+FP)\n",
    "#recall = TP/(TP+FN)\n",
    "#acc = (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2382, 106]\n"
     ]
    }
   ],
   "source": [
    "print([TP, FP, TN, FN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something other than predicting locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'J:\\\\Geology\\\\WSGS\\\\GIS\\\\Predicted locations\\\\'\n",
    "allFiles=glob.glob(path+\"*.shp\")\n",
    "points = gpd.read_file(r'J:\\Geology\\WSGS\\GIS\\usgs_locations.shp')\n",
    "points= points.to_crs({'init': 'epsg:3732'})\n",
    "\n",
    "validations = []\n",
    "for shapefile in allFiles:\n",
    "    shape = gpd.read_file(shapefile)\n",
    "    shape = shape.to_crs({'init': 'epsg:3732'})\n",
    "    chosen = gpd.sjoin(points, shape)\n",
    "    chosen['element'] = shapefile[-10:-4]\n",
    "    validations.append(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedSamples = pd.concat(validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-d353bbd069d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselectedSamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ESRI Shapefile'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'usgs_samples_clipped.shp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_file'"
     ]
    }
   ],
   "source": [
    "selectedSamples.to_file(driver='ESRI Shapefile', filename='usgs_samples_clipped.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ag'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedSamples.iloc[0].element[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse.pisel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (6,10,14,16,23,25,41,63,65,67,71,73,83,85,97,105,109,113,119,133,145,149,163,177,183) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\jesse.pisel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (16,34,52,86,114,124,184) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "path = r'J:\\\\Geology\\\\WSGS\\\\Projects\\\\Critical Minerals\\\\ngdbrock-fUS56\\\\ngdbrock-fUS56\\\\'\n",
    "allFiles=glob.glob(path+\"*.csv\")\n",
    "rocks=pd.DataFrame\n",
    "listed=[]\n",
    "for file in allFiles:\n",
    "    df = pd.read_csv(file)\n",
    "    listed.append(df)\n",
    "rocks = pd.concat(listed, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocks[rocks['lab_id'] == selectedSamples.iloc[50].lab_id].filter(like=selectedSamples.iloc[50].element[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
